{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlgaGeta/info_search/blob/main/HW_3_infsearch_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClQdguYi7OOs",
        "outputId": "e7054819-947a-4dbd-b79c-c95e65b660e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ClIHR-M7bLh",
        "outputId": "5a62d0d9-650b-4c25-fdf2-f569167dc02e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/data\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/MyDrive/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1Hl0Ka87bOz",
        "outputId": "4110201f-ffcb-49e0-88bc-53c26ed6a7ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "I-GDeQIh7bRa"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBpnDszW7bUS",
        "outputId": "a464f99f-ddc6-446e-8602-dd488cb74d4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# предобученная модель BERT для русскоязычных текстов\n",
        "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
        "model = BertModel.from_pretrained('DeepPavlov/rubert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6YODExY8Oo-",
        "outputId": "54b53fbc-46c8-486f-abfe-698e32d984e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.10/dist-packages (0.9.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (0.6.2)\n"
          ]
        }
      ],
      "source": [
        "pip install pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CE4xuSuF8nRl",
        "outputId": "8d6ebf75-a0fd-46f6-c33e-b3f5b79a0336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yQQMdmBz7bY9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "from math import log\n",
        "import pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "feDnSWwHK6sI"
      },
      "outputs": [],
      "source": [
        "source_dir = 'data'\n",
        "morph = pymorphy2.MorphAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_inverted_index(source_dir):\n",
        "    inverted_index = defaultdict(dict)\n",
        "    doc_freq = defaultdict(int)\n",
        "    total_docs = 0\n",
        "\n",
        "    # Подсчет файлов в директории\n",
        "    for filename in os.listdir(source_dir):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            total_docs += 1\n",
        "            file_path = os.path.join(source_dir, filename)\n",
        "\n",
        "            #Разделение текста на части длиной до 512 символов(для BERT-а) и приведение к нижнему регистру\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                text = file.read().lower()\n",
        "                chunks = [text[i:i+512] for i in range(0, len(text), 512)]\n",
        "\n",
        "                '''\n",
        "                Для каждой разделенной части текста производится\n",
        "                токенизация, затем их индексация и создание тензора с индексами\n",
        "                '''\n",
        "                for chunk in chunks:\n",
        "                    tokens = tokenizer.tokenize(chunk)\n",
        "                    indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "                    input_ids = torch.tensor([indexed_tokens])\n",
        "\n",
        "                    #Подача тензора в модель и получение векторного представления\n",
        "                    with torch.no_grad():\n",
        "                        outputs = model(input_ids)\n",
        "                        vector_repr = outputs.last_hidden_state[0].mean(dim=0)\n",
        "\n",
        "                    term_freq = defaultdict(int) # Словарь для хранения частоты терминов в документе\n",
        "                    #Удаление пунктуации, лемматизация\n",
        "                    for token in tokens:\n",
        "                        token = re.sub(r'[^\\w\\s]', '', token)\n",
        "                        lemma = morph.parse(token)[0].normal_form\n",
        "                        term_freq[lemma] += 1\n",
        "\n",
        "                    #Для каждого терма и его частоты. Происходит добавление информации о термине в обратный индекс\n",
        "                    for term, freq in term_freq.items():\n",
        "                        inverted_index[term][filename] = (freq, vector_repr)\n",
        "                        doc_freq[term] += 1\n",
        "\n",
        "    idf = {term: log(total_docs / freq) for term, freq in doc_freq.items()}\n",
        "\n",
        "    for term, doc_freqs in inverted_index.items():\n",
        "        for doc, (freq, vector_repr) in doc_freqs.items():\n",
        "            tfidf = freq * idf[term]\n",
        "            inverted_index[term][doc] = (tfidf, vector_repr)\n",
        "\n",
        "    return inverted_index"
      ],
      "metadata": {
        "id": "JBzHMer0NwNu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_inverted_index(inverted_index, words):\n",
        "    search_results = defaultdict(float)\n",
        "\n",
        "    # обход слов, которые ищем и их лемматизация\n",
        "    for word in words:\n",
        "        lemma = morph.parse(word)[0].normal_form\n",
        "\n",
        "        '''\n",
        "          Итерация по каждому документу, связанному со словом в обратном индексе\n",
        "          tfidf - вес TF-IDF слова в документе, vector_repr - векторное представление документа\n",
        "        '''\n",
        "        for doc, (tfidf, vector_repr) in inverted_index.get(lemma, {}).items():\n",
        "            search_results[doc] += abs(tfidf)\n",
        "\n",
        "    return search_results"
      ],
      "metadata": {
        "id": "4E0eS_d9M7VW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHsj5tTlK6sJ",
        "outputId": "3e10dd66-951b-4ce2-e950-dacee716ee18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Документы с искомыми словами:\n",
            "postmodern.txt: 0.6931471805599453\n"
          ]
        }
      ],
      "source": [
        "inverted_index = build_inverted_index(source_dir)\n",
        "\n",
        "search_words = [\"симпсон\", \"устарел\"]\n",
        "results = search_inverted_index(inverted_index, search_words)\n",
        "\n",
        "if results:\n",
        "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
        "    print(\"Документы с искомыми словами:\")\n",
        "    for doc, score in sorted_results:\n",
        "        print(f\"{doc}: {score}\")\n",
        "else:\n",
        "    print(\"Нет документов с такими словами\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH1g0Ozv9MOW",
        "outputId": "c31667e3-4e72-445f-9fd0-997b87a2c359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Документы с искомыми словами:\n",
            "postmodern.txt: 3.9620029377331667\n",
            "metamodern.txt: 2.204145020180793\n",
            "allmodern.txt: 2.204145020180793\n",
            "modernart.txt: 1.9810014688665833\n"
          ]
        }
      ],
      "source": [
        "search_words = [\"смена\", \"постмодерна\"]\n",
        "results = search_inverted_index(inverted_index, search_words)\n",
        "\n",
        "if results:\n",
        "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
        "    print(\"Документы с искомыми словами:\")\n",
        "    for doc, score in sorted_results:\n",
        "        print(f\"{doc}: {score}\")\n",
        "else:\n",
        "    print(\"Нет документов с такими словами\")\n",
        "#если оба слова не найдены в документе, то при нахождении одного,\n",
        "#файл также выводится в списке, но с более низким значением"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWVlKnIU9MQ7",
        "outputId": "45835c23-2105-4a81-e798-b44be829bd3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Документы с искомыми словами:\n",
            "modernart.txt: 4.329910633534868\n",
            "metamodern.txt: 2.8258332367585934\n",
            "allmodern.txt: 2.8258332367585934\n",
            "postmodern.txt: 1.5040773967762742\n"
          ]
        }
      ],
      "source": [
        "search_words = [\"современное\", \"искусство\"]\n",
        "results = search_inverted_index(inverted_index, search_words)\n",
        "\n",
        "if results:\n",
        "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
        "    print(\"Документы с искомыми словами:\")\n",
        "    for doc, score in sorted_results:\n",
        "        print(f\"{doc}: {score}\")\n",
        "else:\n",
        "    print(\"Нет документов с такими словами\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrURY3SG9MTz",
        "outputId": "f1939a3d-50c6-4d3b-efdb-6ec640a26f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Документы с искомыми словами:\n",
            "metamodern.txt: 0.28768207245178085\n",
            "allmodern.txt: 0.28768207245178085\n",
            "postmodern.txt: 0.28768207245178085\n"
          ]
        }
      ],
      "source": [
        "search_words = [\"традиционализм\", \"современность\"]\n",
        "results = search_inverted_index(inverted_index, search_words)\n",
        "\n",
        "if results:\n",
        "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
        "    print(\"Документы с искомыми словами:\")\n",
        "    for doc, score in sorted_results:\n",
        "        print(f\"{doc}: {score}\")\n",
        "else:\n",
        "    print(\"Нет документов с такими словами\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TC1lX1i9yTj",
        "outputId": "28c88233-b989-42b7-a7cd-0aa727cd24b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Документы с искомыми словами:\n",
            "allmodern.txt: 0.8630462173553426\n",
            "modernart.txt: 0.28768207245178085\n",
            "postmodern.txt: 0.0\n"
          ]
        }
      ],
      "source": [
        "search_words = [\"рефлексируя\", \"над\", \"уже\", \"созданным\"]\n",
        "results = search_inverted_index(inverted_index, search_words)\n",
        "\n",
        "if results:\n",
        "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
        "    print(\"Документы с искомыми словами:\")\n",
        "    for doc, score in sorted_results:\n",
        "        print(f\"{doc}: {score}\")\n",
        "else:\n",
        "    print(\"Нет документов с такими словами\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DewR4J459yQa",
        "outputId": "1b9ee6e9-5fa7-4a10-e3f5-516cd73339b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Документы с искомыми словами:\n",
            "allmodern.txt: 0.8630462173553426\n",
            "modernart.txt: 0.28768207245178085\n",
            "postmodern.txt: 0.0\n"
          ]
        }
      ],
      "source": [
        "search_words = [\"рефлексировать\", \"над\", \"уже\", \"создать\"]\n",
        "results = search_inverted_index(inverted_index, search_words)\n",
        "\n",
        "if results:\n",
        "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
        "    print(\"Документы с искомыми словами:\")\n",
        "    for doc, score in sorted_results:\n",
        "        print(f\"{doc}: {score}\")\n",
        "else:\n",
        "    print(\"Нет документов с такими словами\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}